import discord
from discord.ext import commands
import nest_asyncio
from openai import OpenAI
import re

# Apply nest_asyncio to handle event loop issues in Jupyter Notebooks
nest_asyncio.apply()

# Discord bot token
TOKEN = '<YOUR_DISCORD_BOT_TOKEN>'

# LLaMA API key
LLAMA_API_KEY = '<YOUR_LLAMA_API_KEY>'

# Initialize the OpenAI client for LLaMA API
client = OpenAI(
    api_key=LLAMA_API_KEY,
    base_url="https://api.llama-api.com"
)

# Define intents
intents = discord.Intents.default()
intents.message_content = True  # Enable the message content intent

# Initialize the bot with a command prefix and intents
bot = commands.Bot(command_prefix='!', intents=intents)

@bot.event
async def on_ready():
    print(f'We have logged in as {bot.user}')

@bot.command()
async def ask(ctx, *, question: str):
    # Check for source code related queries
    source_code_patterns = [
        r'source\s*code',       # Matches "source code"
        r'github',              # Matches "github"
        r'code\s*repository',   # Matches "code repository"
        r'project\s*link',      # Matches "project link"
    ]
    
    if any(re.search(pattern, question, re.IGNORECASE) for pattern in source_code_patterns):
        await ctx.send("[Source Code - GitHub](https://github.com/FlameF0X/COL-project)")
        return

    # Define the prompt for the LLaMA model
    prompt = f"Q: {question}\nA:"

    # Make a request to the LLaMA API using the 7b model
    response = client.chat.completions.create(
        model="llama-7b",  # Set to the 7b model
        messages=[
            {"role": "system", "content": "Assistant is a large language model trained by OpenAI."},
            {"role": "user", "content": prompt}
        ]
    )

    # Extract and print the generated response
    generated_text = response.choices[0].message.content.strip()

    # Send the response back to the Discord channel
    await ctx.send(generated_text)

# Run the bot
bot.run(TOKEN)
