# Import required libraries
import discord
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import asyncio
import nest_asyncio

# Set up the Discord client
intents = discord.Intents.default()
intents.message_content = True
client = discord.Client(intents=intents)

# Load the pre-trained DialoGPT-large model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-large")
model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-large")

# Source code link
source_code_link = "-# [Source Code - GitHub](<https://github.com/FlameF0X/COL-project>)"

# Function to chat with the bot without memory
def chat_with_bot(input_text):
    # Encode the input and add special tokens
    input_ids = tokenizer.encode(input_text + tokenizer.eos_token, return_tensors="pt")

    # Generate a response from the model without using chat history
    response_ids = model.generate(input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)

    # Decode the response
    response_text = tokenizer.decode(response_ids[:, input_ids.shape[-1]:][0], skip_special_tokens=True)

    return response_text

# Handle messages
@client.event
async def on_ready():
    print(f'Logged in as {client.user}')

@client.event
async def on_message(message):
    if message.author == client.user:
        return

    if message.content.lower().startswith('!chat'):
        user_input = message.content[5:].strip()
        response = chat_with_bot(user_input)
        response_with_link = f"{response}\n\n{source_code_link}"
        await message.channel.send(response_with_link)

# Function to run the bot
async def run_bot():
    await client.start('YOUR_DISCORD_BOT_TOKEN')  # Replace with your actual bot token

# Run the bot in the notebook's event loop
nest_asyncio.apply()
loop = asyncio.get_event_loop()
loop.run_until_complete(run_bot())
